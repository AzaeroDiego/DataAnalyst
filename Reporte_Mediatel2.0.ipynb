{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha de inicio: 2024-09-01 00:00:00\n",
      "Fecha de fin: 2024-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import json\n",
    "import locale\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import create_engine, text\n",
    "import difflib\n",
    "from dateutil.parser import parse\n",
    "import glob # Para leer archivos de una carpeta\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "import calendar\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar locale para español (Perú)\n",
    "locale.setlocale(locale.LC_TIME, 'es_PE.UTF-8')\n",
    "\n",
    "# Variables PERIODO ABRIL 2024\n",
    "fecha = '202409'\n",
    "\n",
    "# Crear la fecha de inicio del mes concatenando con '01'\n",
    "fecha_inicio = f\"{fecha}01\"\n",
    "fecha_inicio_dt = pd.to_datetime(fecha_inicio, format='%Y%m%d')\n",
    "\n",
    "usuario = 'azaer'\n",
    "\n",
    "# Calcular el último día del mes\n",
    "anio = int(fecha[:4])\n",
    "Mes = int(fecha[4:6])\n",
    "ultimo_dia = calendar.monthrange(anio, Mes)[1]\n",
    "\n",
    "# Crear la fecha de fin del mes\n",
    "fecha_fin = f\"{fecha}{ultimo_dia}\"\n",
    "fecha_fin_dt = pd.to_datetime(fecha_fin, format='%Y%m%d')\n",
    "\n",
    "print(f\"Fecha de inicio: {fecha_inicio_dt}\")\n",
    "print(f\"Fecha de fin: {fecha_fin_dt}\")\n",
    "\n",
    "# Convertir las fechas a cadenas en formato YYYY-MM-DD para usar en SQL\n",
    "fecha_inicio_str = fecha_inicio_dt.strftime('%Y-%m-%d')\n",
    "fecha_fin_str = fecha_fin_dt.strftime('%Y-%m-%d')\n",
    "\n",
    "hoy = dt.datetime.now().strftime('%d%m%Y')\n",
    "\n",
    "# Mes a partir del fecha\n",
    "mes = dt.datetime.strptime(fecha, '%Y%m').strftime('%B').upper()\n",
    "Ruta = rf'C:\\Users\\{usuario}\\Documents\\Diego\\Reportes'\n",
    "Documentos = rf'C:\\Users\\{usuario}\\Documents'\n",
    "Descargas = rf'C:\\Users\\{usuario}\\Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procesamiento de CSV (LLamadas)\n",
    "\n",
    "\n",
    "# Leer archivo de llamadas_{mes} más reciente de la carpeta de descargas\n",
    "archivo = max(glob.glob(f'{Documentos}\\\\llamadas_{mes}.csv'), key=os.path.getctime)\n",
    "df_llamadas = pd.read_csv(archivo, sep=';',dtype = str)\n",
    "\n",
    "#Extracción de Tipificación desde excel\n",
    "df_tipificaciones = pd.read_excel(f'{Documentos}\\\\RESULTANTEVENTASPENDIENTE.xlsx', sheet_name='Hoja1', dtype=str)\n",
    "\n",
    "#Consolidar df_llamadas con df_tipificaciones\n",
    "df_llamadas =pd.concat([df_llamadas, df_tipificaciones], ignore_index=True)\n",
    "\n",
    "# Limpiar la columna 'Agent Name'\n",
    "df_llamadas['Agent Name'] = df_llamadas['Agent Name'].fillna('').apply(lambda x: x.split(',')[0] if ',' in x else x)\n",
    "\n",
    "#Reemplazar valores vacios de CallCode por 'NO CONTESTA\n",
    "df_llamadas['Callcode'] = df_llamadas['Callcode'].fillna('NO CONTESTA / OCUPADO')\n",
    "df_llamadas['Agent Name'] = df_llamadas['Agent Name'].replace('','Resultado Marcador Predictivo')\n",
    "\n",
    "\n",
    "#Filtrar registros innecesarios\n",
    "df_llamadas = df_llamadas[df_llamadas['Callcode'] != 'Default Callcode']\n",
    "df_llamadas = df_llamadas[~df_llamadas['Queue name'].isin(['Outbound_Retenciones_Ripley', 'Regrabacion', 'Inbound_Retenciones_Ripley'])]\n",
    "\n",
    "#Eliminar registros especificos que no son venta\n",
    "NO_VENTA = ['472479','515497','441234' ,'524952','418805','450129','436163','461054','663162', #Junio\n",
    "           '675934', '700552', '704594','779241','774593','768197','785202','835875','835752', #Julio\n",
    "           '912513', '970101', # Agosto\n",
    "           '1057817','1057875']   #Septiembre\n",
    "df_llamadas = df_llamadas[~df_llamadas['CallTraceID'].isin(NO_VENTA)]\n",
    "\n",
    "# donde TraceID es igual a 745788 Cambiar Tipificacion a VENTA\n",
    "df_llamadas.loc[df_llamadas['CallTraceID'].isin(['745788', '864052','879455' ,  #Julio\n",
    "                                                 '927008','1013629']), 'Callcode'] = 'VENTA' #Agosto\n",
    "\n",
    "# Definir el mapeo de tipificaciones de nivel 2 a nivel 1\n",
    "tipificaciones_nivel1 = {\n",
    "    'ND LO LLAMARON MAS DE UNA VEZ': 'EFECTIVO',\n",
    "    'NO DESEA - YA LE OFRECIERON': 'EFECTIVO',\n",
    "    'ND POR COSTO': 'EFECTIVO',\n",
    "    'ND NO TIENE TARJETA': 'EFECTIVO',\n",
    "    'ND COYUNTURAL': 'EFECTIVO',\n",
    "    'ND NO CONFORME RIPLEY': 'EFECTIVO',\n",
    "    'REVALIDACION': 'EFECTIVO',\n",
    "    'VOLVER A LLAMAR': 'EFECTIVO',\n",
    "    'CLIENTE CORTO LLAMADA CON INFO': 'EFECTIVO',\n",
    "    'ND NO BRINDA MOTIVO': 'EFECTIVO',\n",
    "    'ND NO CONTRATA NADA POR TELF.': 'EFECTIVO',\n",
    "    'VENTA': 'EFECTIVO',\n",
    "    'GRABADORA': 'NO CONTACTO',\n",
    "    'BUZON DE VOZ': 'NO CONTACTO',\n",
    "    'NO CONTESTA / OCUPADO': 'NO CONTACTO',\n",
    "    'FUERA DE SERVICIO / SUSPENDIDO': 'NO CONTACTO',\n",
    "    'TITULAR INUBICABLE': 'NO CONTACTO',\n",
    "    'CONTACTO CON TERCERO': 'NO EFECTIVO',\n",
    "    'CORTA LLAMADA SIN INFO': 'NO EFECTIVO',\n",
    "    'NUMERO EQUIVOCADO': 'NO EFECTIVO'\n",
    "}\n",
    "\n",
    "# Mapear las tipificaciones de nivel 2 a nivel 1\n",
    "df_llamadas['Tipificacion_Nivel1'] = df_llamadas['Callcode'].map(tipificaciones_nivel1)\n",
    "\n",
    "# Definir prioridades\n",
    "prioridades = {\n",
    "    'EFECTIVO': 2,\n",
    "    'NO EFECTIVO': 3,\n",
    "    'NO CONTACTO': 4\n",
    "}\n",
    "\n",
    "df_llamadas['Prioridad'] = np.where(df_llamadas['Callcode'] == 'VENTA', 1, df_llamadas['Tipificacion_Nivel1'].map(prioridades).fillna(5))\n",
    "\n",
    "\n",
    "#Formatear Columna Fecha IncomingCallTime a formato fecha (2024-06-13 08:50:41 a 2 columnas de Fecha y hora)\n",
    "df_llamadas['Fecha'] = pd.to_datetime(df_llamadas['IncomingCallTime'], format='mixed').dt.date\n",
    "df_llamadas['Hora'] = pd.to_datetime(df_llamadas['IncomingCallTime'], format='mixed').dt.time\n",
    "\n",
    "#Renombrar columna DNIS por Numero\n",
    "df_llamadas.rename(columns={'DNIS': 'Numero'}, inplace=True)\n",
    "\n",
    "# Ordenar por DNI, Prioridad y Fecha\n",
    "df_llamadas = df_llamadas.sort_values(by=['Numero', 'Prioridad', 'Fecha','Hora'], ascending=[True, True, True, False])\n",
    "\n",
    "# Seleccionar Columnas relevantes\n",
    "df_llamadas = df_llamadas[['Call Type','Numero','Queue name', 'Fecha', 'Hora', 'Tipificacion_Nivel1', 'Callcode','Client talk time', 'Prioridad','CallTraceID','Agent Name']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas eliminadas: 23235\n",
      "Nuevos datos subidos a la base de datos\n"
     ]
    }
   ],
   "source": [
    "# Definir los parámetros de conexión\n",
    "server = r'Diego\\RIPLEYCHUBB'\n",
    "database = 'Reportes_Chubb'\n",
    "schema = 'dbo'\n",
    "username = 'sa'\n",
    "password = 'sanandreas666'\n",
    "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};'\n",
    "\n",
    "# Crear el motor de conexión\n",
    "engine = create_engine(f'mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes')\n",
    "\n",
    "# Crear la conexión\n",
    "connection = pyodbc.connect(connection_string)\n",
    "# Crear un cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Realizar Select a la tabla de clientes para Obtener DNI, Campaña y captación\n",
    "\n",
    "query = f\"\"\"\n",
    "select id,dni,ceL1,ceL2,campana,captación,calL_ID from Reportes_Chubb..Clientes where aniomes = '{fecha}'\n",
    "\"\"\"\n",
    "\n",
    "df_clientes = pd.read_sql(query, connection,dtype=str)\n",
    "\n",
    "# Inicializar las columnas nuevas con NaN\n",
    "df_llamadas['DNI_CLIENTE'] = np.nan\n",
    "df_llamadas['campana'] = np.nan\n",
    "df_llamadas['captación'] = np.nan\n",
    "\n",
    "# Eliminar duplicados en 'ceL1' y 'ceL2' para cada columna relevante\n",
    "df_clientes_ceL1 = df_clientes.drop_duplicates(subset=['ceL1'], keep='first')\n",
    "df_clientes_ceL2 = df_clientes.drop_duplicates(subset=['ceL2'], keep='first')\n",
    "\n",
    "# Ahora realizar el mapeo sin duplicados\n",
    "dni_ceL1 = df_llamadas['Numero'].map(df_clientes_ceL1.set_index('ceL1')['dni'])\n",
    "dni_ceL2 = df_llamadas['Numero'].map(df_clientes_ceL2.set_index('ceL2')['dni'])\n",
    "\n",
    "campana_ceL1 = df_llamadas['Numero'].map(df_clientes_ceL1.set_index('ceL1')['campana'])\n",
    "campana_ceL2 = df_llamadas['Numero'].map(df_clientes_ceL2.set_index('ceL2')['campana'])\n",
    "\n",
    "captacion_ceL1 = df_llamadas['Numero'].map(df_clientes_ceL1.set_index('ceL1')['captación'])\n",
    "captacion_ceL2 = df_llamadas['Numero'].map(df_clientes_ceL2.set_index('ceL2')['captación'])\n",
    "\n",
    "# Utilizar np.where para asignar valores dependiendo de las coincidencias\n",
    "df_llamadas['DNI_CLIENTE'] = np.where(dni_ceL1.notna(), dni_ceL1, dni_ceL2)\n",
    "df_llamadas['campana'] = np.where(campana_ceL1.notna(), campana_ceL1, campana_ceL2)\n",
    "df_llamadas['captación'] = np.where(captacion_ceL1.notna(), captacion_ceL1, captacion_ceL2)\n",
    "\n",
    "# Convertir 'Client talk time' a segundos\n",
    "df_llamadas['Client talk time'] = pd.to_timedelta(df_llamadas['Client talk time']).dt.total_seconds()\n",
    "\n",
    "#Crear columna de TMO_Segundos\n",
    "df_llamadas['TMO_Segundos'] = df_llamadas['Client talk time']\n",
    "\n",
    "# Calcular TMO_Total y Intentos_Total por DNI_CLIENTE\n",
    "df_totales = df_llamadas.groupby('DNI_CLIENTE').agg(\n",
    "    TMO_Total=('Client talk time', 'sum'),\n",
    "    Intentos_Total=('DNI_CLIENTE', 'size')\n",
    ").reset_index()\n",
    "\n",
    "# Convertir TMO_Total de segundos a hh:mm:ss\n",
    "df_totales['TMO_Total'] = pd.to_datetime(df_totales['TMO_Total'], unit='s').dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# Calcular TMO_Total_Dia y Intentos_Dia por DNI_CLIENTE y Fecha\n",
    "df_totales_dia = df_llamadas.groupby(['DNI_CLIENTE', 'Fecha']).agg(\n",
    "    TMO_Total_Dia=('Client talk time', 'sum'),\n",
    "    Intentos_Dia=('DNI_CLIENTE', 'size')\n",
    ").reset_index()\n",
    "\n",
    "# Convertir TMO_Total_Dia de segundos \n",
    "df_totales_dia['TMO_Total_Dia'] = pd.to_datetime(df_totales_dia['TMO_Total_Dia'], unit='s').dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Unir los resultados totales por DNI\n",
    "df_llamadas = df_llamadas.merge(df_totales[['DNI_CLIENTE', 'TMO_Total', 'Intentos_Total']], on='DNI_CLIENTE', how='left')\n",
    "\n",
    "# Unir los resultados totales por DNI y Fecha\n",
    "df_llamadas = df_llamadas.merge(df_totales_dia[['DNI_CLIENTE', 'Fecha', 'TMO_Total_Dia', 'Intentos_Dia']], on=['DNI_CLIENTE', 'Fecha'], how='left')\n",
    "\n",
    "# Asegúrate de que los datos estén ordenados por DNI_CLIENTE, Fecha, y Hora\n",
    "df_llamadas = df_llamadas.sort_values(by=['DNI_CLIENTE', 'Fecha', 'Hora'])\n",
    "\n",
    "# Crear una columna 'FechaHora' combinando 'Fecha' y 'Hora'\n",
    "df_llamadas['FechaHora'] = pd.to_datetime(df_llamadas['Fecha'].astype(str) + ' ' + df_llamadas['Hora'].astype(str))\n",
    "\n",
    "# Calcular la diferencia de tiempo en minutos entre cada intento, agrupado por 'DNI_CLIENTE'\n",
    "df_llamadas['Diferencia_Tiempo'] = df_llamadas.groupby('DNI_CLIENTE')['FechaHora'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# Crear una columna para marcar el inicio de un nuevo trámite\n",
    "df_llamadas['Nuevo_Tramite'] = (df_llamadas['Diferencia_Tiempo'].isna()) | (df_llamadas['Diferencia_Tiempo'] > 60)\n",
    "\n",
    "# Asignar un ID de trámite incremental para cada grupo de trámites\n",
    "df_llamadas['ID_Tramite'] = df_llamadas.groupby('DNI_CLIENTE')['Nuevo_Tramite'].cumsum()\n",
    "\n",
    "# Contar el número de trámites por DNI_CLIENTE\n",
    "df_tramites = df_llamadas.groupby('DNI_CLIENTE').agg(\n",
    "    Intentos_Total=('DNI_CLIENTE', 'size'),\n",
    "    Tramites_Total=('ID_Tramite', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Unir el cálculo de trámites al DataFrame original\n",
    "df_llamadas = df_llamadas.merge(df_tramites[['DNI_CLIENTE', 'Tramites_Total']], on='DNI_CLIENTE', how='left')\n",
    "\n",
    "# Eliminar las columnas: 'Diferencia_Tiempo', 'Nuevo_Tramite' y 2 otras columnas\n",
    "df_llamadas = df_llamadas.drop(columns=['Diferencia_Tiempo', 'Nuevo_Tramite', 'ID_Tramite', 'FechaHora'])\n",
    "\n",
    "#Exportar a excel todas las filas donde dni es nulo\n",
    "df_llamadas[df_llamadas['DNI_CLIENTE'].isnull()].to_excel(f'{Ruta}\\\\Llamadas_sin_dni_{mes}.xlsx', index=False)\n",
    "\n",
    "#Tomando en cuenta la conexión y el cursor, eliminemos los registros de Gestion_Mediatel que coincidan con el rango de inicio y termino mes\n",
    "\n",
    "# Ejecutar la consulta DELETE con parámetros\n",
    "delete_query = \"DELETE FROM dbo.Gestion_Mediatel WHERE Fecha >= ? AND Fecha <= ?\"\n",
    "cursor.execute(delete_query, (fecha_inicio_str, fecha_fin_str))\n",
    "\n",
    "# Confirmar los cambios\n",
    "connection.commit()\n",
    "\n",
    "# Verificar el número de filas afectadas\n",
    "print(f'Filas eliminadas: {cursor.rowcount}')\n",
    "\n",
    "# Insertamos los registros de llamadas en la tabla Gestion_Mediatel\n",
    "\n",
    "try:\n",
    "    # Crear una copia del DataFrame y renombrar columnas solo dentro del bloque try\n",
    "    df_llamadas_temp = df_llamadas.copy()\n",
    "\n",
    "       # Mapeo de nombres de columnas del DataFrame a los nombres de columnas en la tabla SQL\n",
    "    mapeo_columnas = {\n",
    "        'campana': 'Campaña',  # Cambiar 'campana' a 'Campaña'\n",
    "        'Intentos_Total' : 'Cantidad_llamadas',  # Cambiar 'Intentos_Total' a 'Cantidad_llamadas'\n",
    "        'captación': 'Captacion',  # Cambiar 'captación' a 'Captacion'\n",
    "        'TMO_Total': 'tmo',  # Cambiar 'TMO_Total' a 'tmo'\n",
    "    }\n",
    "\n",
    "    # Renombrar las columnas en la copia del DataFrame\n",
    "    df_llamadas_temp.rename(columns=mapeo_columnas, inplace=True)\n",
    "\n",
    "    # Subir los nuevos datos a la tabla de SQL Server sin reemplazar toda la tabla\n",
    "    df_llamadas_temp.to_sql(name='Gestion_Mediatel', con=engine, if_exists='append', index=False)\n",
    "    print('Nuevos datos subidos a la base de datos')\n",
    "except Exception as e:\n",
    "    print(f'Error al subir los nuevos datos: {e}')\n",
    "\n",
    "# Cerrar el cursor solamente\n",
    "cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al intentar acceder a la URL o procesar datos: Invalid URL 'h': No scheme supplied. Perhaps you meant https://h?\n",
      "Filas eliminadas en Adicionales: 0\n",
      "DELETE FROM Reportes_Chubb..adicionales WHERE feC_VENTA like '%202409%'\n",
      "Archivo más reciente: C:\\Users\\azaer\\Downloads\\SEGMENTADOR AGOSTO (17).xlsx actualizado al 2024-09-21 16:35:20\n",
      "0   2024-09-02\n",
      "1   2024-09-02\n",
      "2   2024-09-02\n",
      "3   2024-09-02\n",
      "4   2024-09-02\n",
      "Name: Fecha Venta, dtype: datetime64[ns]\n",
      "0    20240902\n",
      "1    20240902\n",
      "2    20240902\n",
      "3    20240902\n",
      "4    20240902\n",
      "Name: Fecha Venta, dtype: object\n",
      "Nuevos datos subidos a la base de datos\n"
     ]
    }
   ],
   "source": [
    "# Proceso de Subida información de Adicionales,lectura de API adicionales y segmentador\n",
    "\n",
    "# Función para convertir fechas al formato YYYYMMDD\n",
    "def convertir_fecha(fecha):\n",
    "    try:\n",
    "        # Intentar convertir con el formato específico 'dd/mm/yyyy'\n",
    "        fecha_dt = pd.to_datetime(fecha, format='%d/%m/%Y', errors='coerce', dayfirst=True)\n",
    "        if not pd.isna(fecha_dt):\n",
    "            return fecha_dt.strftime('%Y%m%d')\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Intentar convertir con el formato 'YYYY-MM-DD'\n",
    "        fecha_dt = pd.to_datetime(fecha, format='%Y-%m-%d', errors='coerce')\n",
    "        if not pd.isna(fecha_dt):\n",
    "            return fecha_dt.strftime('%Y%m%d')\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Intentar convertir con el formato 'YYYY-MM-DD'\n",
    "        fecha_dt = pd.to_datetime(fecha, format='%d-%m-%Y', errors='coerce')\n",
    "        if not pd.isna(fecha_dt):\n",
    "            return fecha_dt.strftime('%Y%m%d')\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Intentar conversión de formato '26 de diciembre de 1988'\n",
    "        fecha_dt = pd.to_datetime(fecha, format='%d de %B de %Y', errors='coerce')\n",
    "        if not pd.isna(fecha_dt):\n",
    "            return fecha_dt.strftime('%Y%m%d')\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "\n",
    "    # Si no coincide con ningún formato, devuelve la fecha original\n",
    "    return fecha\n",
    "\n",
    "def leer_excel_con_tipos_especificos(url, nombre_hoja, columna_fecha):\n",
    "    # Leer todas las columnas como texto\n",
    "    dtype_dict = {col: str for col in pd.read_excel(url, sheet_name=nombre_hoja, nrows=0).columns}\n",
    "    \n",
    "    # Especificar que la columna de fecha debe ser parseada como fecha\n",
    "    dtype_dict.pop(columna_fecha, None)\n",
    "    \n",
    "    # Leer el archivo Excel con las especificaciones\n",
    "    df = pd.read_excel(url, \n",
    "                       sheet_name=nombre_hoja, \n",
    "                       dtype=dtype_dict, \n",
    "                       parse_dates=[columna_fecha])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "url2 = f'https://app.soluziona.pe/API_QA/Peru/CRM/api/Excel_CRM/CRM/Reporte/Excel/Cliente/Adicionales/{fecha}'\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Realiza las solicitudes a ambas APIs\n",
    "    response2 = list(requests.get(url, timeout=10) for url in url2)\n",
    "    df_adicionales = pd.DataFrame(response2.json())\n",
    "\n",
    "except (requests.exceptions.RequestException, ValueError) as e:\n",
    "    # Si hay un error (como que la URL no responda o error de conexión), crea un DataFrame con datos ficticios\n",
    "    print(f\"Error al intentar acceder a la URL o procesar datos: {e}\")\n",
    "    df_adicionales = pd.DataFrame([{\n",
    "        'clI_ID': '0',\n",
    "        'feC_VENTA': f'{fecha}' + '01',\n",
    "        'tipO_ASEG': 'OTRA RELACIÓN',\n",
    "        \"feC_NACIMIENTO\": \"1983-09-09\",\n",
    "        'parentescO_ID': '0',\n",
    "        \"tipO_DOC_ID\": \"D.N.I.\",\n",
    "        'nrO_DOC': '00000000',\n",
    "        'apE_PATERNO': '0',\n",
    "        'apE_MATERNO': '0',\n",
    "        'clI_ANOMBRE1': '0',\n",
    "        'clI_ANOMBRE2': '0',\n",
    "        'sexo': '0',\n",
    "        'estadO_CIVIL': '0',\n",
    "        'email': '0',\n",
    "        'telefonO_MOVIL': '0',\n",
    "        'campana': 'SONRIE_SEGURO'\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "df_adicionales['feC_VENTA'] = df_adicionales['feC_VENTA'].apply(convertir_fecha)\n",
    "df_adicionales['feC_NACIMIENTO'] = pd.to_datetime(df_adicionales['feC_NACIMIENTO'], format='mixed').dt.strftime('%Y%m%d')\n",
    "\n",
    "# Crear la conexión\n",
    "connection = pyodbc.connect(connection_string)\n",
    "# Crear un cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "delete_query_adicionales = f\"DELETE FROM Reportes_Chubb..adicionales WHERE feC_VENTA like '%{fecha}%'\"\n",
    "cursor.execute(delete_query_adicionales)\n",
    "adicionales_eliminados = cursor.rowcount\n",
    "\n",
    "# Confirmar los cambios\n",
    "connection.commit()\n",
    "\n",
    "print(f'Filas eliminadas en Adicionales: {adicionales_eliminados}')\n",
    "\n",
    "print(delete_query_adicionales)\n",
    "\n",
    "\n",
    "archivos_segmentados = [\n",
    "    archivo for archivo in glob.glob(os.path.join(Descargas, \"*SEGMENTADOR*.xlsx\"))\n",
    "    if not os.path.basename(archivo).startswith('~$')\n",
    "]\n",
    "\n",
    "# Seleccionar el archivo más reciente\n",
    "archivo_mas_reciente = max(archivos_segmentados, key=os.path.getmtime, default=None)\n",
    "# Mostrar la fecha de modificacion del archivo\n",
    "fecha_modificacion = dt.datetime.fromtimestamp(os.path.getmtime(archivo_mas_reciente)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f'Archivo más reciente: {archivo_mas_reciente} actualizado al {fecha_modificacion}')\n",
    "\n",
    "mes = dt.datetime.strptime(fecha, '%Y%m').strftime('%B').upper()\n",
    "\n",
    "segmentador_google = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQE03PGEUQUyYQkqyn5_ZJf7fUTlwgaWpHY8hVVunUbw6Apl12oQ5s2yJX3oVZPSg/pub?output=xlsx'\n",
    "mes = dt.datetime.strptime(fecha, '%Y%m').strftime('%B').upper()\n",
    "segmentador = leer_excel_con_tipos_especificos(segmentador_google, mes, 'Fecha Venta')\n",
    "\n",
    "\n",
    "# Verificar las conversiones\n",
    "print(segmentador['Fecha Venta'].head())\n",
    "\n",
    "segmentador['Fecha Venta'] = pd.to_datetime(segmentador['Fecha Venta']).dt.strftime('%Y%m%d')\n",
    "\n",
    "#segmentador = pd.read_excel(archivo_mas_reciente, sheet_name= f'{mes}', dtype = str)\n",
    "segmentador['N Documento'] = segmentador['N Documento'].str[-8:].str.zfill(8)\n",
    "segmentador = segmentador[['SAP TMK', 'Telefono', 'CALL ID', 'ID', 'Fecha Venta',  'Agente',\n",
    "       'Plan', 'N Documento', 'Nombres', 'Paterno', 'Materno',\n",
    "       'Fecha de Nacimiento' , 'Dirección',\n",
    "       'Departamento', 'Provincia', 'Distrito', 'Email',  'Descripción Producto', 'Prima', 'Fecha Grabación',  'Grabada' ]]\n",
    "    \n",
    "#Formatear Columna Fecha Venta a formato fecha YYYYMMDD desde el Formato DD-MM-YYYY\n",
    "\n",
    "\n",
    "\n",
    "# Intentar convertir las fechas asegurando que se lee el formato correcto 'dd/mm/yyyy'\n",
    "#segmentador['Fecha Venta'] = segmentador['Fecha Venta'].apply(convertir_fecha)\n",
    "\n",
    "\n",
    "segmentador['Fecha de Nacimiento'] = segmentador['Fecha de Nacimiento'].apply(convertir_fecha)\n",
    "\n",
    "# Verificar las conversiones\n",
    "print(segmentador['Fecha Venta'].head())\n",
    "\n",
    "\n",
    "#Agregar columna de VentasTotales\n",
    "segmentador['VentasTotales'] = segmentador['Plan'].map(\n",
    "    lambda x: 2 if pd.notnull(x) and '1' in x else\n",
    "              3 if pd.notnull(x) and '2' in x else\n",
    "              4 if pd.notnull(x) and '3' in x else\n",
    "              1 if pd.notnull(x) and 'titular' in x.lower() and 'adicional' not in x.lower() else\n",
    "              0\n",
    ")\n",
    "\n",
    "# Actualizar la tabla de Adicionales con los datos de segmentador siempre y cuando [N Documento] no se encuentre en la tabla de Adicionales y Plan = 'Adicional'\n",
    "\n",
    "\n",
    "#Quedarme con los dni que estén en segmentador \n",
    "df_clientes = df_clientes[df_clientes['dni'].isin(segmentador['N Documento'])]\n",
    "\n",
    "'''\n",
    "#Actualizar la columna de ID en segmentador con ID de la tabla de clientes usando el CALL ID como llave\n",
    "segmentador = segmentador.drop(columns = ['ID'])\n",
    "segmentador = segmentador.merge(df_clientes[['id','calL_ID']], left_on='CALL ID', right_on='calL_ID', how='left').drop(columns = ['calL_ID'])\n",
    "\n",
    "\n",
    "# Paso 2: Identificar las filas que no se cruzaron correctamente (ID es nulo)\n",
    "sin_id = segmentador[segmentador['id'].isnull()]\n",
    "\n",
    "# Paso 3: Obtener el CALL ID correcto usando el DNI del titular\n",
    "if not sin_id.empty:\n",
    "    # Obtener el CALL ID correcto del titular\n",
    "    titular = df_clientes[['dni', 'calL_ID']].drop_duplicates(subset=['dni'])\n",
    "    sin_id = sin_id.merge(titular, left_on='N Documento', right_on='dni', how='left').drop(columns=['dni'])\n",
    "    \n",
    "    # Crear un diccionario de mapeo de CALL ID erróneo a CALL ID correcto\n",
    "    call_id_mapping = dict(zip(sin_id['CALL ID'], sin_id['calL_ID']))\n",
    "\n",
    "    \n",
    "    # Actualizar el CALL ID en el segmentador usando replace\n",
    "    segmentador['CALL ID'] = segmentador['CALL ID'].replace(call_id_mapping)\n",
    "\n",
    "\n",
    "    # Repetir el cruce inicial con los CALL ID actualizados\n",
    "    segmentador = segmentador.drop(columns=['id'])\n",
    "    segmentador = segmentador.merge(df_clientes[['id', 'calL_ID']], left_on='CALL ID', right_on='calL_ID', how='left').drop(columns=['calL_ID'])\n",
    "\n",
    "# Validar y limpiar los IDs actualizados\n",
    "segmentador['ID'] = segmentador['id'].fillna('ID NO ENCONTRADO')\n",
    "segmentador = segmentador.drop(columns=['id'])\n",
    "'''\n",
    "#Obtener ID de df_clientes si ID es nulo\n",
    "segmentador = segmentador.merge(df_clientes[['id','dni']], left_on='N Documento', right_on='dni', how='left').drop(columns = ['dni'])\n",
    "\n",
    "# Reemplazar todos los valores de ID segmentador por el nuevo ID cliente donde ID sea nulo\n",
    "segmentador['ID'] = segmentador['id'].fillna(segmentador['ID'])\n",
    "\n",
    "#Eliminar columnas innecesarias\n",
    "segmentador = segmentador.drop(columns=['id'])\n",
    "\n",
    "#Reemplazar todos los valores de ID segmentador por el nuevo ID cliente\n",
    "\n",
    "segmentador2 = segmentador[segmentador['Plan'] == 'Adicional']\n",
    "\n",
    "df_adicionales_2 = pd.DataFrame({\n",
    "    'clI_ID' : segmentador2['ID'],\n",
    "    'feC_VENTA' : segmentador2['Fecha Venta'],\n",
    "    'tipO_ASEG' : 'OTRA RELACIÓN',\n",
    "    'feC_NACIMIENTO' : segmentador2['Fecha de Nacimiento'],\n",
    "    'nrO_DOC' : segmentador2['N Documento'],\n",
    "    'apE_PATERNO' : segmentador2['Paterno'],\n",
    "    'apE_MATERNO' : segmentador2['Materno'],\n",
    "    'clI_ANOMBRE1' : segmentador2['Nombres'].str.split(' ').str[0],\n",
    "    'clI_ANOMBRE2' : segmentador2['Nombres'].str.split(' ').str[1],\n",
    "    'sexo' : '',\n",
    "    'estadO_CIVIL' : '',\n",
    "    'email' : segmentador2['Email'],\n",
    "    'telefonO_MOVIL' : segmentador2['Telefono'],    \n",
    "    })\n",
    "\n",
    "\n",
    "# Primer filtro: Filtrar df_adicionales_2 para que solo contenga filas donde el número de documento no esté en df_adicionales\n",
    "df_adicionales_2 = df_adicionales_2[~df_adicionales_2['nrO_DOC'].isin(df_adicionales['nrO_DOC'])]\n",
    "\n",
    "# Concatenar los DataFrames de Adicionales\n",
    "df_adicionales = pd.concat([df_adicionales, df_adicionales_2], ignore_index=True)\n",
    "\n",
    "# Insertar los nuevos datos\n",
    "try:\n",
    "    df_adicionales.to_sql(name='adicionales', con=engine, if_exists='append', index=False)\n",
    "    print('Nuevos datos subidos a la base de datos')\n",
    "except Exception as e:\n",
    "    print(f'Error al subir los nuevos datos: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE FROM Reportes_Chubb..segmentador WHERE [Fecha Venta] like '%202409%'\n",
      "Filas eliminadas en Segmentador: 282\n",
      "Nuevos datos subidos a la base de datos\n"
     ]
    }
   ],
   "source": [
    "#Comienza insersión de Datos en segmentador\n",
    "\n",
    "#Eliminar registros de segmentador que coincidan con el rango de inicio y termino mes\n",
    "\n",
    "delete_query_segmentador = f\"DELETE FROM Reportes_Chubb..segmentador WHERE [Fecha Venta] like '%{fecha}%'\"\n",
    "cursor.execute(delete_query_segmentador)\n",
    "\n",
    "print(delete_query_segmentador)\n",
    "\n",
    "# Confirmar los cambios\n",
    "connection.commit()\n",
    "\n",
    "print(f'Filas eliminadas en Segmentador: {cursor.rowcount}')\n",
    "\n",
    "# Insertar los nuevos datos\n",
    "try:\n",
    "    # Subir los nuevos datos a la tabla de SQL Server sin reemplazar toda la tabla\n",
    "    segmentador.to_sql(name='Segmentador', con=engine, if_exists='append', index=False)\n",
    "    print('Nuevos datos subidos a la base de datos')\n",
    "except Exception as e:\n",
    "    print(f'Error al subir los nuevos datos: {e}')\n",
    "\n",
    "# Ejecutar el procedimiento almacenado desde Python\n",
    "sp_query = f\"EXEC Reportes_Chubb..ObtenerTipificaciones '{fecha}'\"\n",
    "cursor.execute(sp_query)\n",
    "connection.commit()\n",
    "\n",
    "# Cerrar el cursor y la conexión\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado a las  08:03:55\n",
      "['S' 'N' nan]\n"
     ]
    }
   ],
   "source": [
    "print('Proceso terminado a las ', dt.datetime.now().strftime('%H:%M:%S'))\n",
    "print(segmentador['Grabada'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
